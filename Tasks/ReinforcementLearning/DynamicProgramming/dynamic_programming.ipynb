{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于动态规划(Dynamic Programming)\n",
    "\n",
    "- 前提：环境已知，模型已知\n",
    "- 目的：求 最优价值函数 -> 最优策略\n",
    "\n",
    "分为`策略迭代方式`和`价值迭代方式`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略迭代\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_PolicyIteration(RL_Model):\n",
    "  def __init__(self, env:ENV_INFO, theta, gamma):\n",
    "    self.env = env\n",
    "    self.mdp = env.matrix\n",
    "    self.v = [0]*env._states_num\n",
    "    self.pi = [[0]*env._actions_num]*env._states_num\n",
    "    self.theta = theta\n",
    "    self.gamma = gamma\n",
    "\n",
    "  def policy_evaluation(self):\n",
    "    cnt = 1\n",
    "    while 1:\n",
    "      max_diff = 0\n",
    "      new_v = [0]*self.env._states_num\n",
    "      for s in range(self.env._states_num):\n",
    "        Q = []\n",
    "        for a in range(self.env._actions_num):\n",
    "          q = 0\n",
    "          for next_s in range(self.env._states_num):\n",
    "            p = self.mdp.P[s][a][next_s]\n",
    "            r = self.mdp.R_E[next_s]\n",
    "            done = self.mdp.done[next_s]\n",
    "            q += p*(r+self.gamma*self.v[next_s]*(1-done))\n",
    "          Q.append(self.pi[s][a]*q)\n",
    "        new_v[s] = sum(Q)\n",
    "        max_diff = max(max_diff, abs(new_v[s]-self.v[s]))\n",
    "      self.v = new_v\n",
    "      if max_diff < self.theta:\n",
    "        break\n",
    "      cnt += 1\n",
    "    print(f'{cnt}轮后完成 policy_evaluation ')\n",
    "  \n",
    "  def policy_improvement(self):\n",
    "    for s in range(self.env._states_num):\n",
    "      Q = []\n",
    "      for a in range(self.env._actions_num):\n",
    "        q = 0\n",
    "        for next_s in range(self.env._states_num):\n",
    "          p = self.mdp.P[s][a][next_s]\n",
    "          r = self.mdp.R_E[next_s]\n",
    "          done = self.mdp.done[next_s]\n",
    "          q += p*(r+self.gamma*self.v[next_s]*(1-done))\n",
    "        Q.append(q)\n",
    "      maxQ = max(Q)\n",
    "      cntQ = Q.count(maxQ)\n",
    "      self.pi[s] = [1/cntQ if q == maxQ else 0 for q in Q]\n",
    "    return self.pi\n",
    "  \n",
    "  def run(self):\n",
    "    cnt = 0\n",
    "    while 1:\n",
    "      self.policy_evaluation()\n",
    "      old_pi = copy.deepcopy(self.pi)\n",
    "      new_pi = self.policy_improvement()\n",
    "      if old_pi == new_pi:\n",
    "        cnt += 1\n",
    "        if cnt > 5:\n",
    "          break\n",
    "      else:\n",
    "        cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 价值迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
