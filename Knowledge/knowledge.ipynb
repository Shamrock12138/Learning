{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e6a155",
   "metadata": {},
   "source": [
    "### 记录知识点\n",
    "\n",
    "#### 为什么神经网络的训练集要满足独立同分布（i.i.d., independent and identically distributed）假设？\n",
    "\n",
    "  * i.i.d\n",
    "\n",
    "    * 独立（Independent）：  \n",
    "      指样本之间统计上相互独立，任一样本的取值 **不依赖也不影响** 其他样本取值。\n",
    "      例：从洗匀的牌堆里有放回地抽牌\n",
    "\n",
    "    * 同分布（Identically Distributed）：  \n",
    "      指所有样本都来自 **同一个概率分布** 。\n",
    "      例：所有采样都从标准正态分布中反复采样\n",
    "      反例：不同的设备采集到的毫米波信号/训练时使用干净语音，测试用嘈杂语音。\n",
    "\n",
    "  * 为什么要满足i.i.d？\n",
    "\n",
    "    * 独立性 - 保证样本间无信息泄露\n",
    "\n",
    "    * 同分布 - 保证训练和测试“看到的世界是一样的”\n",
    "\n",
    "\n",
    "\n",
    "#### softmax策略\n",
    "\n",
    "  给定一个动作价值函数估计器 $\\hat{Q}(s, a, g; \\theta)$，我们可以构造一个以目标 $g$ 为条件的策略（即目标条件化策略）：\n",
    "\n",
    "  $$\n",
    "  \\pi_{\\hat{Q}}^{\\tau}(a \\mid s, g) := \\frac{\\exp\\left( \\hat{Q}(s, a, g; \\theta) / \\tau \\right)}{\\sum_{a'} \\exp\\left( \\hat{Q}(s, a', g; \\theta) / \\tau \\right)}\n",
    "  $$\n",
    "\n",
    "  * τ 温度系数；  \n",
    "    控制探索程度 τ→0 时退化为贪婪策略\n",
    "\n",
    "#### SVD矩阵分解\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
