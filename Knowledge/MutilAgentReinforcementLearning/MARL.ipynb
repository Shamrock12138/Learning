{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Reinforcement Learning（MARL，多智能体强化学习）\n",
    "\n",
    "**分类**\n",
    "\n",
    "- 完全中心化（fully centralized）：\n",
    "\n",
    "  将多个智能体进行决策当作一个超级智能体在进行决策，即把所有智能体的状态聚合在一起当作一个全局的超级状态，把所有智能体的动作连起来作为一个联合动作。\n",
    "\n",
    "- 完全去中心化（fully decaentralized）：\n",
    "\n",
    "  与完全中心化方法相反的范式便是假设每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变。\n",
    "\n",
    "**区别**\n",
    "\n",
    "- 完全中心化的好处是，由于已经知道了所有智能体的状态和动作，因此对这个超级智能体来说，环境依旧是稳态的，一些单智能体的算法的收敛性依旧可以得到保证。然而，这样的做法不能很好地扩展到智能体数量很多或者环境很大的情况，因为这时候将所有的信息简单暴力地拼在一起会导致维度爆炸，*训练复杂度巨幅提升的问题往往不可解决*。\n",
    "\n",
    "- 完全去中心化方法直接对每个智能体用一个单智能体强化学习算法来学习。这样做的缺点是环境是非稳态的，训练的收敛性不能得到保证，但是这种方法的好处在于随着智能体数量的增加有比较好的扩展性，*不会遇到维度灾难而导致训练不能进行下去*。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
