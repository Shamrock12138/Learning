{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8e47f3",
   "metadata": {},
   "source": [
    "## 马尔可夫（MDP）决策过程\n",
    "\n",
    "- 状态价值函数\n",
    "\n",
    "  状态价值函数 $V^{\\pi}(s)$ 表示在策略 $\\pi$ 下，从状态 $s$ 开始所能获得的期望累积回报。\n",
    "\n",
    "  **公式：**\n",
    "    $$V^{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n",
    "\n",
    "  其中：\n",
    "  - $\\pi$ 是策略\n",
    "  - $s$ 是状态\n",
    "  - $\\gamma$ 是折扣因子\n",
    "  - $R_{t+k+1}$ 是时刻 $t+k+1$ 的奖励\n",
    "\n",
    "- 动作价值函数\n",
    "\n",
    "  动作价值函数 $Q^{\\pi}(s, a)$ 表示在策略 $\\pi$ 下，从状态 $s$ 执行动作 $a$ 后所能获得的期望累积回报。\n",
    "\n",
    "  **公式：**\n",
    "  $$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$\n",
    "\n",
    "  其中：\n",
    "  - $a$ 是动作\n",
    "  - $A_t$ 是时刻 $t$ 的动作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b491615",
   "metadata": {},
   "source": [
    "## 贝尔曼期望方程\n",
    "\n",
    "- 状态价值函数的贝尔曼方程\n",
    "\n",
    "  **公式：**\n",
    "  $$V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma V^{\\pi}(s')\\right]$$\n",
    "  \n",
    "  ![alt text](57f729de048db7dd85a92c19d12103da.jpg)\n",
    "\n",
    "- 动作价值函数的贝尔曼方程\n",
    "\n",
    "  **公式：**\n",
    "  $$Q^{\\pi}(s, a) = \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma \\sum_{a' \\in A} \\pi(a' \\mid s') Q^{\\pi}(s', a')\\right]$$\n",
    "\n",
    "  ![alt text](3d1f13bb494a1183dd4ef1a133d68442.jpg)\n",
    "\n",
    "- 两个价值函数之间的关系\n",
    "\n",
    "  **从状态价值到动作价值：**\n",
    "  $$Q^{\\pi}(s, a) = \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma V^{\\pi}(s')\\right]$$\n",
    "\n",
    "  **从动作价值到状态价值：**\n",
    "  $$V^{\\pi}(s) = \\sum_{a \\in A} \\pi(a \\mid s) Q^{\\pi}(s, a)$$\n",
    "\n",
    "  其中：\n",
    "  - $P(s' \\mid s, a)$ 是状态转移概率\n",
    "  - $R(s, a, s')$ 是即时奖励\n",
    "  - $\\pi(a \\mid s)$ 是策略概率\n",
    "  - $S$ 是状态空间\n",
    "  - $A$ 是动作空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb38375",
   "metadata": {},
   "source": [
    "## 贝尔曼最优方程\n",
    "\n",
    "- 状态价值函数的贝尔曼最优方程\n",
    "\n",
    "    最优状态价值函数 $V^*(s)$ 表示在所有可能的策略中，从状态 $s$ 开始所能获得的最大期望累积回报。\n",
    "\n",
    "    **公式：**\n",
    "    $$V^*(s) = \\max_{\\pi} V^{\\pi}(s)$$\n",
    "\n",
    "    $$V^*(s) = \\max_{a \\in A} \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma V^*(s')\\right]$$\n",
    "\n",
    "- 动作价值函数的贝尔曼最优方程\n",
    "\n",
    "  最优动作价值函数 $Q^*(s, a)$ 表示在所有可能的策略中，从状态 $s$ 执行动作 $a$ 后所能获得的最大期望累积回报。\n",
    "\n",
    "  **公式：**\n",
    "  $$Q^*(s, a) = \\max_{\\pi} Q^{\\pi}(s, a)$$\n",
    "\n",
    "  $$Q^*(s, a) = \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma \\max_{a' \\in A} Q^*(s', a')\\right]$$\n",
    "\n",
    "- 最优策略\n",
    "\n",
    "  基于最优价值函数，最优策略 $\\pi^*$ 可以表示为：\n",
    "\n",
    "  **确定性最优策略：**\n",
    "  $$\\pi^*(s) = \\arg\\max_{a \\in A} Q^*(s, a)$$\n",
    "\n",
    "  **或等价地：**\n",
    "  $$\\pi^*(s) = \\arg\\max_{a \\in A} \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma V^*(s')\\right]$$\n",
    "\n",
    "- 最优价值函数之间的关系\n",
    "\n",
    "  $$Q^*(s, a) = \\sum_{s' \\in S} P(s' \\mid s, a) \\left[R(s, a, s') + \\gamma V^*(s')\\right]$$\n",
    "\n",
    "  $$V^*(s) = \\max_{a \\in A} Q^*(s, a)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
