{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd37563",
   "metadata": {},
   "source": [
    "### Double DQN(DDQN) 介绍\n",
    "\n",
    "  **关键问题**：当Q网络对某些状态-动作对的价值估计存在噪声或误差时，最大化操作会倾向于选择那些被高估的动作，即使它们的实际价值并不高。\n",
    "\n",
    "  **标准DQN存在Q值过估计（Overestimation）问题**：\n",
    "  - DQN使用相同的Q网络来**选择动作**和**评估动作价值**\n",
    "  - 由于最大化操作（$\\max_{a} Q(s_{t+1}, a)$）和函数近似的结合，Q值会被系统性高估\n",
    "\n",
    "  Double DQN通过**解耦动作选择和价值评估**来解决过估计问题：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa45da1",
   "metadata": {},
   "source": [
    "#### Double DQN的核心思想\n",
    "\n",
    "  将**动作选择**和**Q值评估**分离到两个不同的网络：\n",
    "  - **在线网络（Online Network）**：负责选择最优动作\n",
    "  - **目标网络（Target Network）**：负责评估所选动作的Q值\n",
    "\n",
    "  **标准DQN的TD目标**：\n",
    "  $$y_t^{\\text{DQN}} = r_{t+1} + \\gamma \\max_{a} Q_{\\text{target}}(s_{t+1}, a; \\theta^-)$$\n",
    "\n",
    "  **Double DQN的TD目标**：\n",
    "  $$y_t^{\\text{DDQN}} = r_{t+1} + \\gamma \\cdot Q_{\\text{target}}\\left(s_{t+1}, \\arg\\max_{a} Q_{\\text{online}}(s_{t+1}, a; \\theta); \\theta^-\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f55799e",
   "metadata": {},
   "source": [
    "> 🤔神经网络的过估计问题源于“随机”，即对于Q(s, a)，某个Q(s, ai)可能莫名其妙变大，那么可不可以添加一种“置信概率”P，用来判断Q(s, ai)此时值的可信度，即Q(s, ai)的真实值是PiQ(s, ai)，再以此为依据采取greedy策略，即选择max(PiQ(s, ai))，或者选择sum(PiQ(s, ai))，此时既不牺牲稳定性，又保证了不会出现过估计问题。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
