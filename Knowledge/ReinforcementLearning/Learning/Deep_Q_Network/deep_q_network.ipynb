{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0c5178",
   "metadata": {},
   "source": [
    "### 深度Q网络（deep Q network, DQN）\n",
    "\n",
    "DQN（Deep Q-Network）是一种将**深度学习**与**Q-learning**相结合的强化学习算法，由 DeepMind 在 2013 年提出（2015 年 Nature 版改进），首次在 Atari 游戏中达到人类水平表现。\n",
    "\n",
    "#### 核心思想\n",
    "\n",
    "- 使用**深度神经网络**（如 CNN）近似动作价值函数 $ Q(s, a; \\theta) $\n",
    "- 通过**经验回放**（Experience Replay）打破样本相关性，提升训练稳定性\n",
    "- 使用**目标网络**（Target Network）固定目标值更新，缓解自举（bootstrapping）带来的不稳定性\n",
    "\n",
    "#### 损失函数（MSE）\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\left[ \\left( y - Q(s, a; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "其中  \n",
    "$$\n",
    "y = \n",
    "\\begin{cases}\n",
    "r & \\text{if } s' \\text{ is terminal} \\\\\n",
    "r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### 优势与局限\n",
    "\n",
    "**优势**  \n",
    "- 适用于高维状态空间（如图像输入）  \n",
    "- 端到端训练，无需手动设计特征  \n",
    "- 理论上收敛到最优策略（在满足条件下）\n",
    "\n",
    "**局限**  \n",
    "- 仅适用于**离散动作空间**  \n",
    "- Q 值易过估计（后续有 Double DQN 改进）  \n",
    "- 对超参数（如 replay buffer 大小、target 更新频率）较敏感  \n",
    "- 训练不稳定、样本效率较低\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
